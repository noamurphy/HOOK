{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Requirements"
      ],
      "metadata": {
        "id": "EJPx7SJAeGTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets[audio] torchaudio_augmentations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZX9bSdYeML5",
        "outputId": "89e0179e-6104-4916-faf2-4d249e34c1d5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets[audio] in /usr/local/lib/python3.9/dist-packages (2.11.0)\n",
            "Requirement already satisfied: torchaudio_augmentations in /usr/local/lib/python3.9/dist-packages (0.2.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets[audio]) (2.27.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from datasets[audio]) (0.14.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets[audio]) (23.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from datasets[audio]) (0.18.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets[audio]) (3.2.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets[audio]) (9.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets[audio]) (1.5.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets[audio]) (1.22.4)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets[audio]) (2023.4.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets[audio]) (3.8.4)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from datasets[audio]) (0.3.6)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets[audio]) (0.70.14)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets[audio]) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets[audio]) (6.0)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.9/dist-packages (from datasets[audio]) (0.12.1)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.9/dist-packages (from datasets[audio]) (0.10.0.post2)\n",
            "Requirement already satisfied: wavaugment in /usr/local/lib/python3.9/dist-packages (from torchaudio_augmentations) (0.2)\n",
            "Requirement already satisfied: torch-pitch-shift in /usr/local/lib/python3.9/dist-packages (from torchaudio_augmentations) (1.2.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from torchaudio_augmentations) (1.9.0)\n",
            "Requirement already satisfied: julius in /usr/local/lib/python3.9/dist-packages (from torchaudio_augmentations) (0.2.7)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.9/dist-packages (from torchaudio_augmentations) (0.9.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets[audio]) (1.9.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets[audio]) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets[audio]) (1.3.3)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets[audio]) (2.0.12)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets[audio]) (1.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets[audio]) (6.0.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets[audio]) (23.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets[audio]) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets[audio]) (3.11.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets[audio]) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets[audio]) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets[audio]) (1.26.15)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.9/dist-packages (from soundfile>=0.12.1->datasets[audio]) (1.15.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.9/dist-packages (from librosa->datasets[audio]) (4.4.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.9/dist-packages (from librosa->datasets[audio]) (1.2.0)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.9/dist-packages (from librosa->datasets[audio]) (0.2)\n",
            "Requirement already satisfied: pooch<1.7,>=1.0 in /usr/local/lib/python3.9/dist-packages (from librosa->datasets[audio]) (1.6.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.9/dist-packages (from librosa->datasets[audio]) (1.2.2)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.9/dist-packages (from librosa->datasets[audio]) (3.0.0)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.9/dist-packages (from librosa->datasets[audio]) (0.56.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.9/dist-packages (from librosa->datasets[audio]) (1.0.5)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from librosa->datasets[audio]) (1.10.1)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.9/dist-packages (from librosa->datasets[audio]) (0.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets[audio]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets[audio]) (2022.7.1)\n",
            "Requirement already satisfied: primePy>=1.3 in /usr/local/lib/python3.9/dist-packages (from torch-pitch-shift->torchaudio_augmentations) (1.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.0->soundfile>=0.12.1->datasets[audio]) (2.21)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba>=0.51.0->librosa->datasets[audio]) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from numba>=0.51.0->librosa->datasets[audio]) (67.7.1)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.9/dist-packages (from pooch<1.7,>=1.0->librosa->datasets[audio]) (1.4.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->datasets[audio]) (1.16.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.20.0->librosa->datasets[audio]) (3.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "lssxL_lads8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchaudio\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class GTZAN(Dataset):\n",
        "\n",
        "    def __init__(self, split=\"train\"):\n",
        "        self.dataset = load_dataset(\"marsyas/gtzan\", split=split)\n",
        "        self.labels = ['blues', 'classical', 'country', 'disco', 'hiphop',\n",
        "                       'jazz', 'metal', 'pop', 'reggae', 'rock']\n",
        "\n",
        "        self.label2idx = {label: idx for idx, label in enumerate(self.labels)}\n",
        "        self.n_classes = len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_path = self.dataset[idx]['file']\n",
        "        audio, sr = torchaudio.load(file_path)\n",
        "        label = self.label2idx[self.labels[self.dataset[idx]['genre']]]\n",
        "\n",
        "        return audio, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)"
      ],
      "metadata": {
        "id": "cP2mulhQdx8Z"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Wrapper for Torch Dataset class to enable contrastive training\n",
        "\"\"\"\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.utils.data import Dataset\n",
        "from torchaudio_augmentations import Compose\n",
        "from typing import Tuple, List\n",
        "\n",
        "\n",
        "class ContrastiveDataset(Dataset):\n",
        "    def __init__(self, dataset: Dataset, input_shape: List[int], transform: Compose):\n",
        "        self.dataset = dataset\n",
        "        self.transform = transform\n",
        "        self.input_shape = input_shape\n",
        "        self.ignore_idx = []\n",
        "\n",
        "    def __getitem__(self, idx) -> Tuple[Tensor, Tensor]:\n",
        "        if idx in self.ignore_idx:\n",
        "            return self[idx + 1]\n",
        "\n",
        "        audio, label = self.dataset[idx]\n",
        "\n",
        "        if audio.shape[1] < self.input_shape[1]:\n",
        "            self.ignore_idx.append(idx)\n",
        "            return self[idx + 1]\n",
        "\n",
        "        if self.transform:\n",
        "            audio = self.transform(audio)\n",
        "        return audio, label\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def concat_clip(self, n: int, audio_length: float) -> Tensor:\n",
        "        audio, _ = self.dataset[n]\n",
        "        batch = torch.split(audio, audio_length, dim=1)\n",
        "        batch = torch.cat(batch[:-1])\n",
        "        batch = batch.unsqueeze(dim=1)\n",
        "\n",
        "        if self.transform:\n",
        "            batch = self.transform(batch)\n",
        "\n",
        "        return batch"
      ],
      "metadata": {
        "id": "y82-4niSeHF1"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torchaudio_augmentations import(ComposeMany, RandomResizedCrop)\n",
        "\n",
        "def get_dataset(dataset):\n",
        "    train_transform = [RandomResizedCrop(n_samples=59049)]\n",
        "    num_augmented_samples = 1\n",
        "    if dataset == \"gtzan\":\n",
        "        d = GTZAN()\n",
        "        contrastive_dataset = ContrastiveDataset(\n",
        "        d,\n",
        "        input_shape=(1, 59049),\n",
        "        transform=ComposeMany(train_transform, num_augmented_samples)\n",
        "    )\n",
        "    else:\n",
        "        raise NotImplementedError(\"Dataset not implemented\")\n",
        "    return contrastive_dataset"
      ],
      "metadata": {
        "id": "5xSXbegkdsQy"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataset tests"
      ],
      "metadata": {
        "id": "VqWhBdabefFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unittest\n",
        "import torch\n",
        "\n",
        "class TestGTZAN(unittest.TestCase):\n",
        "    def test_dataset(self):\n",
        "        dataset = GTZAN()\n",
        "        sample_idx = 0\n",
        "        sample = dataset.__getitem__(sample_idx)\n",
        "\n",
        "        # Audio waveform\n",
        "        self.assertIsInstance(sample[0], torch.Tensor)\n",
        "\n",
        "        # Label\n",
        "        self.assertIsInstance(sample[1], int)\n",
        "\n",
        "        # Audio waveform has at least 1 sample\n",
        "        self.assertGreaterEqual(sample[0].shape[0], 1)\n",
        "\n",
        "        # Label is non-negative\n",
        "        self.assertGreaterEqual(sample[1], 0)\n",
        "        \n",
        "        # Label is less than the number of classes in the dataset\n",
        "        self.assertLess(sample[1], dataset.num_classes)"
      ],
      "metadata": {
        "id": "uCuV74qsel6o"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gtzan_test = TestGTZAN()\n",
        "gtzan_test.test_dataset"
      ],
      "metadata": {
        "id": "LGOZWAHpgx2W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11f80b27-dd2d-431f-c8f0-1ca9b16434ca"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method TestGTZAN.test_dataset of <__main__.TestGTZAN testMethod=runTest>>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataloader"
      ],
      "metadata": {
        "id": "5AF1eOS8jKwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "dataset = get_dataset(\"gtzan\")\n",
        "\n",
        "dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=48,\n",
        "    num_workers=0,\n",
        "    drop_last=True,\n",
        "    shuffle=False,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAvHLQFmmNaz",
        "outputId": "3306e815-6bce-4b0f-866d-4d3681a53e95"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset gtzan (/root/.cache/huggingface/datasets/marsyas___gtzan/default/0.0.0/8bd0e23c2d9b2be30d36bc6834319772dff22a3bd28527996612386cef003910)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataloader tests"
      ],
      "metadata": {
        "id": "5WzQ1qZR4rm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TestDataLoader(unittest.TestCase):\n",
        "    \n",
        "    def test_batch_size(self):\n",
        "        # set up\n",
        "        dataset = get_dataset(\"gtzan\")\n",
        "        input_shape = (1, 59049)\n",
        "        dataloader = DataLoader(dataset, batch_size=48, num_workers=0, drop_last=True, shuffle=False)\n",
        "        \n",
        "        # test\n",
        "        for batch_idx, (data, target) in enumerate(dataloader):\n",
        "            batch_size = data.shape[0]\n",
        "            self.assertEqual(batch_size, 48, f\"Batch {batch_idx} - Expected batch size: 48, Actual batch size: {batch_size}\")"
      ],
      "metadata": {
        "id": "arwwKyefnqnP"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataloader_test = TestDataLoader()\n",
        "# dataloader_test.test_batch_size()"
      ],
      "metadata": {
        "id": "k_r8QjRYnw4z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "outputId": "9951f46f-3277-4013-d58f-e0f85fb580aa"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset gtzan (/root/.cache/huggingface/datasets/marsyas___gtzan/default/0.0.0/8bd0e23c2d9b2be30d36bc6834319772dff22a3bd28527996612386cef003910)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-88e0808e9281>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdataloader_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTestDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataloader_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-42-9de9f312a83c>\u001b[0m in \u001b[0;36mtest_batch_size\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massertEqual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m48\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Batch {batch_idx} - Expected batch size: 48, Actual batch size: {batch_size}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [1, 661794] at entry 0 and [1, 669680] at entry 46"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "1YsJOL4adff-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "    def initialize(self, m):\n",
        "        if isinstance(m, (nn.Conv1d)):\n",
        "            # nn.init.xavier_uniform_(m.weight)\n",
        "            # if m.bias is not None:\n",
        "            #     nn.init.xavier_uniform_(m.bias)\n",
        "\n",
        "            nn.init.kaiming_uniform_(m.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
        "\n",
        "\n",
        "class Identity(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Identity, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x"
      ],
      "metadata": {
        "id": "XmbEqSbGsYbr"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SampleCNN(Model):\n",
        "    def __init__(self, strides):\n",
        "        super(SampleCNN, self).__init__()\n",
        "\n",
        "        self.strides = strides\n",
        "        self.sequential = [\n",
        "            nn.Sequential(\n",
        "                nn.Conv1d(1, 128, kernel_size=3, stride=3, padding=0),\n",
        "                nn.BatchNorm1d(128),\n",
        "                nn.ReLU(),\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        self.hidden = [\n",
        "            [128, 128],\n",
        "            [128, 128],\n",
        "            [128, 256],\n",
        "            [256, 256],\n",
        "            [256, 256],\n",
        "            [256, 256],\n",
        "            [256, 256],\n",
        "            [256, 256],\n",
        "            [256, 512],\n",
        "        ]\n",
        "\n",
        "        assert len(self.hidden) == len(\n",
        "            self.strides\n",
        "        ), \"Number of hidden layers and strides are not equal\"\n",
        "        for stride, (h_in, h_out) in zip(self.strides, self.hidden):\n",
        "            self.sequential.append(\n",
        "                nn.Sequential(\n",
        "                    nn.Conv1d(h_in, h_out, kernel_size=stride, stride=1, padding=1),\n",
        "                    nn.BatchNorm1d(h_out),\n",
        "                    nn.ReLU(),\n",
        "                    nn.MaxPool1d(stride, stride=stride),\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # 1 x 512\n",
        "        self.sequential.append(\n",
        "            nn.Sequential(\n",
        "                nn.Conv1d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "                nn.BatchNorm1d(512),\n",
        "                nn.ReLU(),\n",
        "            )\n",
        "        )\n",
        "\n",
        "        self.sequential = nn.Sequential(*self.sequential)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x[:, 0, :]\n",
        "        out = self.sequential(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "gh5yRFGbsVy5"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "def load_encoder_checkpoint(checkpoint_path: str) -> OrderedDict:\n",
        "    state_dict = torch.load(checkpoint_path, map_location=torch.device(\"cpu\"))\n",
        "    new_state_dict = OrderedDict()\n",
        "    for k, v in state_dict.items():\n",
        "        if \"encoder.\" in k:\n",
        "            new_state_dict[k.replace(\"encoder.\", \"\")] = v\n",
        "\n",
        "    return new_state_dict"
      ],
      "metadata": {
        "id": "RlRlo4FEqiAd"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "strides = [3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
        "model = SampleCNN(strides)\n",
        "checkpoint_path = \"clmr_checkpoint_10000.pt\"\n",
        "model.load_state_dict(load_encoder_checkpoint(checkpoint_path))\n",
        "with open('model.pkl', 'wb') as f:\n",
        "  pickle.dump(model, f)"
      ],
      "metadata": {
        "id": "jL0-4YPqaBHp"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()  # put the model in evaluation mode\n",
        "\n",
        "all_representations = []\n",
        "with torch.no_grad():\n",
        "    for data, target in dataloader:\n",
        "        # pass the data through the model to get representations\n",
        "        representations = model(data)\n",
        "        all_representations.append(representations)\n",
        "\n",
        "all_representations = torch.cat(all_representations)\n",
        "with open('representations.pkl', 'wb') as f:\n",
        "  pickle.dump(all_representations, f)"
      ],
      "metadata": {
        "id": "o2t3iQEgwIoe"
      },
      "execution_count": 39,
      "outputs": []
    }
  ]
}